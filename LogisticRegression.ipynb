{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cf4e0f",
   "metadata": {},
   "source": [
    "# Logistic Regression <br>\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. <br>\n",
    "### Types:\n",
    "- Binary (Pass/Fail)\n",
    "- Multi (Cats, Dogs, Sheep)\n",
    "- Ordinal: Multiple with labels/classes related (Low, Medium, High) : \n",
    "\n",
    "### Linear vs Logistic:\n",
    "- Linear Regression could help us predict the students' test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).\n",
    "- Logistic Regression could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the models' classifications.\n",
    "\n",
    "### Sigmoid Function/Activation\n",
    "The function maps any real value into another value between 0 and 1. \n",
    "$$f(x) = \\frac{1} {1 + e^{-x}}$$\n",
    "\n",
    "\n",
    "- y = f(x) = output between 0 and 1 (probability estimate)\n",
    "- x = input to the function\n",
    "- e = 2.71828. Euler's number\n",
    "\n",
    "<img src=\"pics/logis.pic1.png\" width=\"500\">\n",
    "\n",
    "### Decision boundary\n",
    "A threshold where we decide the label for data points depend on the probability from the sigmoid function.<br>\n",
    "For example, $ threshold = 0.5$:\n",
    "$$\\begin{split}p \\geq 0.5, class=1 \\\\\n",
    "p < 0.5, class=0\\end{split}$$\n",
    "\n",
    "### From features to label\n",
    "First we compute z from our features\n",
    "$$z = b + w_1 x_1 + w_2 x_2  = W X + b $$\n",
    "Then, map z into probability using sigmoid function\n",
    "$$P(z) = \\frac{1} {1 + e^{-z}}$$\n",
    "$$P(z) = h(X) = \\frac{1} {1 + e^{-W X + b}}$$\n",
    "Finally, compare $P(z)$ with threshold, to return the label/class\n",
    "### Cost/Loss function\n",
    "We use a cost function called Cross-Entropy, also known as Log Loss\n",
    "<img src=\"pics/logis.pic2.png\" width=\"300\">\n",
    "$h(x) : 0 -> 1$ <br>\n",
    "$y=1: Cost(h(x), y): +inf -> 0 $ <br>\n",
    "$y=0: Cost(h(x), y): 0 -> +inf $\n",
    "<img src=\"pics/logis.pic3.png\" width=\"600\">\n",
    "\n",
    "#### Final Cost function for both cases\n",
    "<img src=\"pics/logis.pic4.png\" width=\"450\">\n",
    "\n",
    "#### Vectorized cost function\n",
    "<img src=\"pics/logis.pic5.png\" width=\"400\">\n",
    "\n",
    "# Gradient descent\n",
    "<img src=\"pics/logis.pic6.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "183ef121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:16:38.220200Z",
     "start_time": "2022-02-08T20:16:38.204308Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr=0.0001, iters=500):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for i_ in range(self.iters):\n",
    "            z = np.dot(X, self.W) + self.b # 1D array\n",
    "            y_hat = 1/(1+np.exp(-z))\n",
    "            dW = np.dot(X.T, (y_hat-y))*2/n_samples #1D array\n",
    "            db = sum((y_hat-y))*2/n_samples #scalar\n",
    "            \n",
    "            self.W -= self.lr*dW\n",
    "            self.b -= self.lr*db\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b # 1D array\n",
    "        y_hat = 1/(1+np.exp(-z))\n",
    "        return np.array([1 if py>=0.5 else 0 for py in y_hat ])\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        z = np.dot(X, self.W) + self.b # 1D array\n",
    "        y_hat = 1/(1+np.exp(-z))\n",
    "        costs = np.dot((1-y),np.log(1-y_hat))*(1-y)-np.dot(y,np.log(y_hat))\n",
    "        return f'{np.mean(costs):.2f}'\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        score = [a==b for (a,b) in zip(y_hat,y)].count(True)/len(y)\n",
    "        return f'{score:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "432b8fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:18:06.410530Z",
     "start_time": "2022-02-08T20:18:06.318640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: \n",
      "[1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 0]\n",
      "y_hat: \n",
      "[1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
      " 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0\n",
      " 0 0 0]\n",
      "Accuracy Score: 0.92\n",
      "Loss: 2.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size= 0.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f'y_test: \\n{y_test}')\n",
    "print(f'y_hat: \\n{model.predict(X_test)}')\n",
    "print(f'Accuracy Score: {model.score(X_test, y_test)}')\n",
    "print(f'Loss: {model.cost(X_test, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
