{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5369ba9",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "## Naive Bayes Theorem\n",
    "<img src=\"pics/nb.pic1.png\" width=\"250\">\n",
    "\n",
    "## Apply for X anf y\n",
    "<img src=\"pics/nb.pic2.png\" width=\"250\">\n",
    "\n",
    "with <img src=\"pics/nb.pic3.png\" width=\"250\">\n",
    "we have <img src=\"pics/nb.pic4.png\" width=\"600\">\n",
    "with <img src=\"pics/nb.pic5.png\" width=\"400\">\n",
    "$$P(x_i|y)= f(x)={\\frac {exp({-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}})}\n",
    "{\\sigma {\\sqrt {2\\pi }}}}$$\n",
    "follow by distribution graph <img src=\"pics/nb.pic6.png\" width=\"400\">\n",
    "## Types of Naive Bayes Classifier\n",
    "### Bernoulli/Binomial Naive Bayes: (Yes- No | 0 - 1 | True - False )\n",
    "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "### Multinomial Naive Bayes: (sports, politics, technology,...)\n",
    "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
    "\n",
    "### Gaussian Naive Bayes:\n",
    "When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6c453bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T22:55:17.371797Z",
     "start_time": "2022-02-17T22:55:17.357637Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features =  X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.varian = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.prior = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            Xc = X[y==cls] # filter only rows has the same class label\n",
    "            self.mean[cls,:] = Xc.mean(axis=0)\n",
    "            self.varian[cls,:] = Xc.var(axis=0)\n",
    "            self.prior[cls] = len(Xc)/n_samples\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_hat = np.array([self.predict_one(x) for x in X])\n",
    "        return y_hat\n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            log_prior = np.log(self.prior[idx])\n",
    "            sum_log_Pxy= sum(np.log(self.gauss_pdf(idx,x)))\n",
    "            posterior = log_prior + sum_log_Pxy\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "            \n",
    "    def gauss_pdf(self, cls_idx, x):\n",
    "        mean = self.mean[cls_idx]\n",
    "        var = self.varian[cls_idx]\n",
    "        numerator = np.exp(-((x-mean)**2/(2*var)))\n",
    "        denominator = np.sqrt(2*np.pi*var)\n",
    "        pdf = numerator/denominator\n",
    "        return pdf\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return f'{sum(yhat==y)/len(y):.3f}'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "54b48744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T22:54:21.957564Z",
     "start_time": "2022-02-17T22:54:21.943572Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib as plt\n",
    "\n",
    "X,y = datasets.make_classification(n_samples=1000,\n",
    "                                  n_features=10,\n",
    "                                  n_classes=2,\n",
    "                                  random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e23efc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T22:55:18.839606Z",
     "start_time": "2022-02-17T22:55:18.808152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: \n",
      "[0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
      " 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0\n",
      " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
      " 1 0 1 0]\n",
      "y_hat: \n",
      "[0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
      " 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0\n",
      " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
      " 1 0 1 0]\n",
      "Accuracy Score: 0.967\n"
     ]
    }
   ],
   "source": [
    "clfmodel = NaiveBayes()\n",
    "clfmodel.fit(X_train, y_train)\n",
    "print(f'y_test: \\n{y_test}')\n",
    "print(f'y_hat: \\n{clfmodel.predict(X_test)}')\n",
    "print(f'Accuracy Score: {clfmodel.accuracy(X_test, y_test)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
